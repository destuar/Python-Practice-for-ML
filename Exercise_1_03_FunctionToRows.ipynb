{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Medium blog post:\n",
    "https://towardsdatascience.com/apply-function-to-pandas-dataframe-rows-76df74165ee4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Ways to Apply a Function to Each Row in Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Faker in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (23.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from Faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   task_name  1000000 non-null  object\n",
      " 1   due_date   1000000 non-null  object\n",
      " 2   priority   1000000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 22.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Generate a DataFrame using Faker with a million tasks. \n",
    "# Each task is a row in the DataFrame. It consists of task_name (str), due_date (datetime.date), and priority (str). \n",
    "# Priority can be one of the three values: LOW, MEDIUM, HIGH\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "task_name = [fake.name() for _ in range(1000000)]\n",
    "due_date = [fake.date() for _ in range(1000000)]\n",
    "priority = [random.choice(['LOW', 'MEDIUM', 'HIGH']) for _ in range(1000000)]\n",
    "\n",
    "test_data_set = pd.DataFrame({'task_name': task_name, 'due_date': due_date, 'priority': priority})\n",
    "\n",
    "test_data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize DataFrame Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of str, priority can be stored as Pandas categorical type\n",
    "priority_dtype = pd.api.types.CategoricalDtype(\n",
    "  categories=['LOW', 'MEDIUM', 'HIGH'],\n",
    "  ordered=True\n",
    ")\n",
    "test_data_set['priority'] = test_data_set['priority'].astype(priority_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count    Dtype   \n",
      "---  ------     --------------    -----   \n",
      " 0   task_name  1000000 non-null  object  \n",
      " 1   due_date   1000000 non-null  object  \n",
      " 2   priority   1000000 non-null  category\n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 16.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Let’s check out the DataFrame size now\n",
    "test_data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eisenhower Action Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given importance and urgency, eisenhower_action computes an integer value between 0 and 3\n",
    "def eisenhower_action(is_important: bool, is_urgent: bool) -> int:\n",
    "  return 2 * is_important + is_urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this exercise, we will assume that a task with HIGH priority is important\n",
    "# If the due date is in the next two days, then the task is urgent\n",
    "# The Eisenhower Action for a task (i.e. a row in the DataFrame) is computed by using the due_date and priority columns\n",
    "cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "\n",
    "eisenhower_action(\n",
    "  test_data_set.loc[0].priority == 'HIGH',\n",
    "  test_data_set.loc[0].due_date <= str(cutoff_date)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will evaluate 12 alternatives for applying eisenhower_action function to DataFrame rows\n",
    "    # First, we will measure the time for a sample of 100k rows\n",
    "    # Then, we will measure and plot the time for up to a million rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1. Loop Over All Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest method to process each row in the good old Python loop. This is obviously the worst way, and nobody in the right mind will ever do it\n",
    "def loop_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  result = []\n",
    "  for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    result.append(\n",
    "      eisenhower_action(\n",
    "        row.priority == 'HIGH', row.due_date <= str(cutoff_date))\n",
    "    )\n",
    "  return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 s ± 455 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_loop'] = loop_impl(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: line_profiler in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.086536 s\n",
      "File: /var/folders/cz/dg_zmrb96h979_06qv2zwrf40000gn/T/ipykernel_26907/4164450296.py\n",
      "Function: loop_impl at line 2\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     2                                           def loop_impl(df):\n",
      "     3         1     678000.0 678000.0      0.8    cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
      "     4         1          0.0      0.0      0.0    result = []\n",
      "     5       101     111000.0   1099.0      0.1    for i in range(len(df)):\n",
      "     6       100   64676000.0 646760.0     74.7      row = df.iloc[i]\n",
      "     7       200     245000.0   1225.0      0.3      result.append(\n",
      "     8       200     313000.0   1565.0      0.4        eisenhower_action(\n",
      "     9       100   12039000.0 120390.0     13.9          row.priority == 'HIGH', row.due_date <= str(cutoff_date))\n",
      "    10                                               )\n",
      "    11         1    8474000.0    8e+06      9.8    return pd.Series(result)"
     ]
    }
   ],
   "source": [
    "# Let’s find out what is taking so long using the line_profiler and %lprun, but for a smaller sample of 100 rows\n",
    "%load_ext line_profiler\n",
    "%lprun -f loop_impl loop_impl(data_sample[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2. Iterate over rows with iterrows Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of processing each row in a Python loop, let’s try Pandas iterrows function\n",
    "def iterrows_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return pd.Series(\n",
    "    eisenhower_action(\n",
    "      row.priority == 'HIGH', row.due_date <= str(cutoff_date))\n",
    "    for index, row in df.iterrows()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.36 s ± 242 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_iterrows'] = iterrows_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3. Iterate over rows with itertuples Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas has another method, itertuples, that processes rows as tuples\n",
    "def itertuples_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return pd.Series(\n",
    "    eisenhower_action(\n",
    "      row.priority == 'HIGH', row.due_date <= str(cutoff_date))\n",
    "    for row in df.itertuples()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 ms ± 42.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_itertuples'] = itertuples_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4. Pandas apply Function to every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame apply function is quite versatile and is a popular choice. To make it process the rows, you have to pass axis=1 argument\n",
    "def apply_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return df.apply(\n",
    "      lambda row:\n",
    "        eisenhower_action(\n",
    "          row.priority == 'HIGH', row.due_date <= str(cutoff_date)),\n",
    "      axis=1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 s ± 103 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_apply'] = apply_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5. Python List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A column in DataFrame is a Series that can be used as a list in a list comprehension expression\n",
    "''' [ foo(x) for x in df['x'] ] '''\n",
    "# If multiple columns are needed, then zip can be used to make a list of tuples\n",
    "def list_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return pd.Series([\n",
    "    eisenhower_action(priority == 'HIGH', due_date <= str(cutoff_date))\n",
    "    for (priority, due_date) in zip(df['priority'], df['due_date'])\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 ms ± 4.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_list'] = list_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6. Python map Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python’s map function that takes in function and iterables of parameters, and yields results\n",
    "def map_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return pd.Series(\n",
    "    map(eisenhower_action,\n",
    "      df['priority'] == 'HIGH',\n",
    "      df['due_date'] <= str(cutoff_date))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 ms ± 6.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_map'] = map_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real power of Pandas shows up in vectorization. But it requires unpacking the function as a vector expression\n",
    "def vec_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return (\n",
    "    2*(df['priority'] == 'HIGH') + (df['due_date'] <= str(cutoff_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.7 ms ± 972 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_vec'] = vec_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 8. NumPy vectorize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.25.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy offers alternatives for migrating from Python to Numpy through vectorization. \n",
    "# For example, it has a vectorize() function that vectorzie any scalar function to accept and return NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "def np_vec_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return np.vectorize(eisenhower_action)(\n",
    "    df['priority'] == 'HIGH',\n",
    "    df['due_date'] <= str(cutoff_date)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7 ms ± 12.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_np_vec'] = np_vec_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 9. Numba Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.59.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.42.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from numba) (1.25.2)\n",
      "Downloading numba-0.59.0-cp311-cp311-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.42.0-cp311-cp311-macosx_10_9_x86_64.whl (31.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.1/31.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.42.0 numba-0.59.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numba is commonly used to speed up applying mathematical functions. It has various decorators for JIT compilation and vectorization\n",
    "import numba\n",
    "\n",
    "@numba.vectorize\n",
    "def eisenhower_action(is_important: bool, is_urgent: bool) -> int:\n",
    "  return 2 * is_important + is_urgent\n",
    "def numba_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return eisenhower_action(\n",
    "    (df['priority'] == 'HIGH').to_numpy(),\n",
    "    (df['due_date'] <= str(cutoff_date)).to_numpy()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.4 ms ± 9.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_numba'] = numba_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 10. Multiprocessing with pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandarallel\n",
      "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dill>=0.3.1 (from pandarallel)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandarallel) (2.1.0)\n",
      "Requirement already satisfied: psutil in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from pandarallel) (5.9.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1->pandarallel) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from pandas>=1->pandarallel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1->pandarallel) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1->pandarallel) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.16.0)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pandarallel\n",
      "  Building wheel for pandarallel (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16672 sha256=3cad700159e37377a15139d07224223dd2af33657c1dbfd51077b574828dbac9\n",
      "  Stored in directory: /Users/diegoestuar/Library/Caches/pip/wheels/b9/c6/5a/829298789e94348b81af52ab42c19d49da007306bbcc983827\n",
      "Successfully built pandarallel\n",
      "Installing collected packages: dill, pandarallel\n",
      "Successfully installed dill-0.3.8 pandarallel-1.6.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# The pandarallel package utilizes multiple CPUs and split the work into multiple threads\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "def pandarallel_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return df.parallel_apply(\n",
    "    lambda row: eisenhower_action(\n",
    "      row.priority == 'HIGH', row.due_date <= str(cutoff_date)),\n",
    "    axis=1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17 s ± 187 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_pandarallel'] = pandarallel_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 11. Parallelize with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2024.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting click>=8.1 (from dask)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=1.5.0 (from dask)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from dask) (23.1)\n",
      "Collecting partd>=1.2.0 (from dask)\n",
      "  Downloading partd-1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.3.1 (from dask)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting locket (from partd>=1.2.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading dask-2024.2.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading partd-1.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl (187 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.9/187.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: zipp, toolz, pyyaml, locket, fsspec, cloudpickle, click, partd, importlib-metadata, dask\n",
      "Successfully installed click-8.1.7 cloudpickle-3.0.0 dask-2024.2.1 fsspec-2024.2.0 importlib-metadata-7.0.1 locket-1.0.0 partd-1.4.1 pyyaml-6.0.1 toolz-0.12.1 zipp-3.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask is a parallel computing library that supports scaling up NumPy, Pandas, Scikit-learn, and many other Python libraries. \n",
    "# It offers efficient infra for processing a massive amount of data on multi-node clusters\n",
    "import dask.dataframe as dd\n",
    "def dask_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return dd.from_pandas(df, npartitions=2).apply(\n",
    "    lambda row: eisenhower_action(\n",
    "      row.priority == 'HIGH', row.due_date <= str(cutoff_date)),\n",
    "    axis=1,\n",
    "    meta=(int)\n",
    "  ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dask/dataframe/core.py:7365: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55 s ± 14.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_dask'] = dask_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 12. Opportunistic Parallelization with Swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from swifter) (2.2.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from swifter) (5.9.5)\n",
      "Requirement already satisfied: dask>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask[dataframe]>=2.10.0->swifter) (2024.2.1)\n",
      "Collecting tqdm>=4.33.0 (from swifter)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (7.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.0.0->swifter) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.0.0->swifter) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
      "Requirement already satisfied: locket in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from partd>=1.2.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16506 sha256=44dc391ce75becbbac5a56ec0cf6b5bde650c473f1389482ebf42839e03c29bf\n",
      "  Stored in directory: /Users/diegoestuar/Library/Caches/pip/wheels/ef/7f/bd/9bed48f078f3ee1fa75e0b29b6e0335ce1cb03a38d3443b3a3\n",
      "Successfully built swifter\n",
      "Installing collected packages: tqdm, swifter\n",
      "Successfully installed swifter-1.4.0 tqdm-4.66.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swifter automatically decides which is faster: to use Dask parallel processing or a simple Pandas apply. \n",
    "# It is very simple to use: just all one word to how one uses Pandas apply function: df.swifter.apply\n",
    "import swifter\n",
    "\n",
    "def swifter_impl(df):\n",
    "  cutoff_date = datetime.date.today() + datetime.timedelta(days=2)\n",
    "  return df.swifter.apply(\n",
    "    lambda row: eisenhower_action(\n",
    "      row.priority == 'HIGH', row.due_date <= str(cutoff_date)),\n",
    "    axis=1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.9 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time for a sample of 100k rows\n",
    "data_sample = test_data_set.sample(100000)\n",
    "%timeit data_sample['action_swifter'] = swifter_impl(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison of Iterating over Pandas DataFrame rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting perfplot\n",
      "  Downloading perfplot-0.10.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from perfplot) (3.8.1)\n",
      "Collecting matplotx (from perfplot)\n",
      "  Downloading matplotx-0.3.10-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from perfplot) (1.25.2)\n",
      "Collecting rich (from perfplot)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from matplotlib->perfplot) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->perfplot) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from matplotlib->perfplot) (2.8.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->perfplot)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from rich->perfplot) (2.16.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->perfplot)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->perfplot) (1.16.0)\n",
      "Downloading perfplot-0.10.2-py3-none-any.whl (21 kB)\n",
      "Downloading matplotx-0.3.10-py3-none-any.whl (25 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, markdown-it-py, rich, matplotx, perfplot\n",
      "Successfully installed markdown-it-py-3.0.0 matplotx-0.3.10 mdurl-0.1.2 perfplot-0.10.2 rich-13.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install perfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/diegoestuar/Library/Python/3.11/lib/python/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m939.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba6990ad7dc4fa3af78a810dee274bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Plotting is helpful in understanding the relative performance of alternatives over input size. \n",
    "Perfplot is a handy tool for that. It requires a setup to generate input of a given size and a list of implementations to compare '''\n",
    "import perfplot\n",
    "\n",
    "kernels = [\n",
    "  loop_impl,\n",
    "  iterrows_impl,\n",
    "  itertuples_impl,\n",
    "  apply_impl,\n",
    "  list_impl,\n",
    "  vec_impl,\n",
    "  np_vec_impl,\n",
    "  numba_impl,\n",
    "  pandarallel_impl,\n",
    "  dask_impl,\n",
    "  swifter_impl\n",
    "]\n",
    "\n",
    "perfplot.show(\n",
    "  setup=lambda n: test_data_set.sample(10000),\n",
    "  kernels=kernels,\n",
    "  n_range=[2 ** k for k in range(1, 21)],\n",
    "  xlabel='n_rows',\n",
    "  logx=True,\n",
    "  logy=True,\n",
    "  equality_check=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing an operation independently to all Pandas rows is a common need. Here are my recommendations:\n",
    "\n",
    "    Vectorize DataFrame expression: Go for this whenever possible.\n",
    "\n",
    "    NumPy vectorize: Its API is not very complicated. It does not require additional packages. It offers almost the best performance. Choose this if vectorizing DataFrame isn’t infeasible.\n",
    "\n",
    "    List Comprehension: Opt for this alternative when needing only 2–3 DataFrame columns, and DataFrame vectorization and NumPy vectorize not infeasible for some reason.\n",
    "\n",
    "    Pandas itertuples function: Its API is like apply function, but offers 10x better performance than apply. It is the easiest and most readable option. It offers reasonable performance. Do this if the previous three do not work out.\n",
    "\n",
    "    Numba or Swift: Use this to exploit parallelization without code complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
